# dp-spark-utils

Package Python utilitaire pour les opÃ©rations PySpark sur Cloudera CDP 7.1.9 avec intÃ©gration Hive/HDFS.

## ðŸ“‹ Description

`dp-spark-utils` est un package Python conÃ§u pour standardiser et simplifier les opÃ©rations courantes avec PySpark dans un environnement Cloudera CDP 7.1.9. Il fournit un ensemble de fonctions utilitaires pour :

- OpÃ©rations sur HDFS (vÃ©rification de fichiers, listing, dÃ©placement)
- OpÃ©rations sur Hive (vÃ©rification de tables, rÃ©cupÃ©ration de colonnes)
- Manipulation de DataFrames (chargement, repartition, Ã©criture, renommage de colonnes)
- Gestion des schÃ©mas et types de donnÃ©es
- Utilitaires de dates
- Validation de donnÃ©es et de fichiers

## ðŸ”§ PrÃ©requis

- Python 3.9 ou 3.10
- PySpark 3.3.2
- Cloudera CDP 7.1.9

## ðŸ“¦ Installation

### Installation depuis le dÃ©pÃ´t Git

```bash
# Cloner le dÃ©pÃ´t
git clone https://gitlab.internal/data-platform/dp-spark-utils.git
cd dp-spark-utils

# Installation en mode dÃ©veloppement
pip install -e .

# Ou installation classique
pip install .
```

### Installation avec les dÃ©pendances de dÃ©veloppement

```bash
pip install -e ".[dev]"
```

### Installation depuis requirements.txt

```bash
pip install -r requirements.txt
```

## ðŸš€ Utilisation

### Import du package

```python
# Import de toutes les fonctions
from dp_spark_utils import (
    get_hadoop_fs,
    check_file_exists,
    check_table_exists,
    load_dataframe,
    map_spark_type,
)

# Ou import par module
from dp_spark_utils.hdfs import check_file_exists, hdfs_list_files
from dp_spark_utils.hive import check_table_exists, get_columns_map
from dp_spark_utils.dataframe import load_dataframe, write_dataframe_csv
from dp_spark_utils.schema import map_spark_type
from dp_spark_utils.date import get_last_day_of_previous_month
from dp_spark_utils.validation import validate_filename_pattern
```

### Exemples d'utilisation

#### OpÃ©rations HDFS

```python
from pyspark.sql import SparkSession
from dp_spark_utils.hdfs import check_file_exists, hdfs_list_files, move_files

spark = SparkSession.builder.enableHiveSupport().getOrCreate()

# VÃ©rifier si un fichier existe
if check_file_exists(spark, "/data/input/file.json", extension=".json"):
    print("Le fichier JSON existe")

# Lister les fichiers CSV d'un rÃ©pertoire
csv_files = hdfs_list_files(spark, "/data/input/", extension=".csv")
print(f"Fichiers CSV trouvÃ©s: {csv_files}")

# DÃ©placer des fichiers CSV
moved = move_files(spark, "/data/temp/", "/data/output/", extension=".csv")
print(f"Fichiers dÃ©placÃ©s: {moved}")
```

#### OpÃ©rations Hive

```python
from dp_spark_utils.hive import check_table_exists, get_columns_map

# VÃ©rifier si une table existe
if check_table_exists(spark, "ma_base", "ma_table"):
    print("La table existe")

# Obtenir les colonnes d'une table
columns, columns_map = get_columns_map(spark, "ma_base", "ma_table")
print(f"Colonnes: {columns}")
print(f"Map des colonnes (insensible Ã  la casse): {columns_map}")
```

#### OpÃ©rations DataFrame

```python
from dp_spark_utils.dataframe import (
    load_dataframe,
    repartition_dataframe,
    write_dataframe_csv,
    rename_columns,
)

# Charger une table Hive
df = load_dataframe(spark, "ma_base", "ma_table", columns=["id", "nom", "email"])

# Repartitionner selon le nombre de lignes par fichier
df, total_rows, partitions = repartition_dataframe(df, lines_per_file=100000)

# Ã‰crire en CSV
write_dataframe_csv(df, "/data/output/export", separator=";", encoding="ISO-8859-1")

# Renommer des colonnes
mapping = [
    {"source": "id", "destination": "user_id"},
    {"source": "nom", "destination": "user_name"},
]
df = rename_columns(df, mapping)
```

#### OpÃ©rations sur les types

```python
from dp_spark_utils.schema import map_spark_type, get_ordered_columns_from_schema

# Mapper un type string vers un type Spark
spark_type = map_spark_type("bigint")  # Retourne LongType()

# Obtenir l'ordre des colonnes depuis un schÃ©ma
schema = [
    {"source": "id", "destination": "user_id"},
    {"source": "name", "destination": "user_name"},
]
ordered_cols = get_ordered_columns_from_schema(schema)  # ['user_id', 'user_name']
```

#### Utilitaires de dates

```python
from dp_spark_utils.date import (
    get_last_day_of_previous_month,
    get_last_day_of_previous_two_months,
)

# Dernier jour du mois prÃ©cÃ©dent
last_day = get_last_day_of_previous_month()  # Format: "20240131"

# Avec format personnalisÃ©
last_day = get_last_day_of_previous_month(date_format="%Y-%m-%d")  # "2024-01-31"

# Dernier jour de deux mois avant
two_months_ago = get_last_day_of_previous_two_months()
```

#### Validation

```python
from dp_spark_utils.validation import validate_columns_match, validate_filename_pattern

# Valider que les colonnes correspondent
source_cols = ["id", "name", "email"]
target_cols = ["ID", "Name", "Email"]
is_match, extra, missing = validate_columns_match(source_cols, target_cols)

# Valider le pattern d'un nom de fichier
is_valid = validate_filename_pattern(
    "20240131_export.csv",
    r"^\d{8}_export\.csv$"
)
```

## ðŸ“ Structure du projet

```
dp-spark-utils/
â”œâ”€â”€ dp_spark_utils/
â”‚   â”œâ”€â”€ __init__.py           # Point d'entrÃ©e du package
â”‚   â”œâ”€â”€ hdfs/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ operations.py     # OpÃ©rations HDFS
â”‚   â”œâ”€â”€ hive/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ operations.py     # OpÃ©rations Hive
â”‚   â”œâ”€â”€ dataframe/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ operations.py     # OpÃ©rations DataFrame
â”‚   â”œâ”€â”€ schema/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ operations.py     # OpÃ©rations sur les schÃ©mas
â”‚   â”œâ”€â”€ date/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ operations.py     # Utilitaires de dates
â”‚   â””â”€â”€ validation/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ operations.py     # Fonctions de validation
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py           # Configuration pytest et fixtures
â”‚   â”œâ”€â”€ test_hdfs.py
â”‚   â”œâ”€â”€ test_hive.py
â”‚   â”œâ”€â”€ test_dataframe.py
â”‚   â”œâ”€â”€ test_schema.py
â”‚   â”œâ”€â”€ test_date.py
â”‚   â”œâ”€â”€ test_validation.py
â”‚   â””â”€â”€ test_init.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml
â”œâ”€â”€ CHANGELOG.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ requirements-dev.txt
```

## ðŸ§ª Tests

### ExÃ©cuter tous les tests

```bash
# Avec couverture de code
pytest

# Sans couverture
pytest --no-cov

# Tests verbeux
pytest -v

# Un fichier de test spÃ©cifique
pytest tests/test_hdfs.py
```

### Couverture de code

Le projet est configurÃ© pour maintenir une couverture de code d'au moins 80%.

```bash
# GÃ©nÃ©rer le rapport de couverture HTML
pytest --cov=dp_spark_utils --cov-report=html

# Voir le rapport dans le terminal
pytest --cov=dp_spark_utils --cov-report=term-missing
```

## ðŸ”¨ Build du package

### Build manuel

```bash
# Installer les outils de build
pip install build twine

# CrÃ©er les distributions
python -m build

# Les fichiers seront crÃ©Ã©s dans dist/
# - dp_spark_utils-0.1.0-py3-none-any.whl
# - dp_spark_utils-0.1.0.tar.gz
```

### VÃ©rification du package

```bash
# VÃ©rifier le package avec twine
twine check dist/*
```

## ðŸ”„ Pre-commit hooks

Le projet utilise pre-commit pour assurer la qualitÃ© du code.

### Installation

```bash
# Installer pre-commit
pip install pre-commit

# Installer les hooks
pre-commit install
```

### Utilisation

```bash
# ExÃ©cuter les hooks manuellement sur tous les fichiers
pre-commit run --all-files

# ExÃ©cuter un hook spÃ©cifique
pre-commit run black --all-files
pre-commit run flake8 --all-files
```

### Hooks configurÃ©s

- **trailing-whitespace** : Supprime les espaces en fin de ligne
- **end-of-file-fixer** : Assure une ligne vide en fin de fichier
- **check-yaml** : Valide la syntaxe YAML
- **check-toml** : Valide la syntaxe TOML
- **black** : Formatage du code Python
- **isort** : Tri des imports
- **flake8** : Linting du code
- **mypy** : VÃ©rification des types

## ðŸ¤ Contribution

### Comment contribuer

1. **Cloner le dÃ©pÃ´t**
   ```bash
   git clone https://gitlab.internal/data-platform/dp-spark-utils.git
   cd dp-spark-utils
   ```

2. **CrÃ©er une branche**
   ```bash
   git checkout -b feature/ma-nouvelle-fonctionnalite
   ```

3. **Installer les dÃ©pendances de dÃ©veloppement**
   ```bash
   pip install -e ".[dev]"
   pre-commit install
   ```

4. **Faire vos modifications**
   - Ã‰crire le code
   - Ã‰crire les tests unitaires
   - S'assurer que les tests passent
   - S'assurer que la couverture est >= 80%

5. **Valider avec pre-commit**
   ```bash
   pre-commit run --all-files
   ```

6. **Commit et push**
   ```bash
   git add .
   git commit -m "feat: description de la fonctionnalitÃ©"
   git push origin feature/ma-nouvelle-fonctionnalite
   ```

7. **CrÃ©er une Merge Request**

### Ajouter une nouvelle fonction

1. **Identifier le module appropriÃ©**
   - `hdfs/` : OpÃ©rations sur HDFS
   - `hive/` : OpÃ©rations sur Hive
   - `dataframe/` : Manipulations de DataFrame
   - `schema/` : Gestion des types et schÃ©mas
   - `date/` : Utilitaires de dates
   - `validation/` : Fonctions de validation

2. **Ajouter la fonction dans le fichier `operations.py` du module**
   ```python
   def ma_nouvelle_fonction(param1: str, param2: int) -> bool:
       """
       Description courte de la fonction.

       Description dÃ©taillÃ©e si nÃ©cessaire.

       Args:
           param1 (str): Description du premier paramÃ¨tre.
           param2 (int): Description du second paramÃ¨tre.

       Returns:
           bool: Description de ce qui est retournÃ©.

       Example:
           >>> ma_nouvelle_fonction("test", 42)
           True
       """
       # Implementation
       return True
   ```

3. **Exporter la fonction dans `__init__.py` du module**
   ```python
   from dp_spark_utils.module.operations import ma_nouvelle_fonction

   __all__ = [
       "ma_nouvelle_fonction",
       # ... autres fonctions
   ]
   ```

4. **Exporter au niveau du package** (optionnel, pour les fonctions principales)
   Dans `dp_spark_utils/__init__.py`:
   ```python
   from dp_spark_utils.module import ma_nouvelle_fonction

   __all__ = [
       "ma_nouvelle_fonction",
       # ... autres fonctions
   ]
   ```

5. **Ã‰crire les tests unitaires**
   Dans `tests/test_module.py`:
   ```python
   class TestMaNouvelleFonction:
       """Tests for ma_nouvelle_fonction."""

       def test_cas_nominal(self):
           """Test the normal use case."""
           result = ma_nouvelle_fonction("test", 42)
           assert result is True

       def test_cas_erreur(self):
           """Test error handling."""
           # ...
   ```

6. **Mettre Ã  jour le CHANGELOG.md**

### Conventions de code

- **Docstrings** : En anglais, format Google/Numpy
- **Nommage** : snake_case pour les fonctions et variables
- **Types** : Utiliser les annotations de type Python
- **Tests** : Un fichier de test par module

## ðŸ“ Logging

### Comportement du logging

Ce package utilise le module standard `logging` de Python avec la convention `logging.getLogger(__name__)` pour chaque module. **Cela signifie que le package ne configure aucun handler, formatter ou niveau de log par dÃ©faut**.

Cette approche garantit que :
- **Pas d'impact sur votre systÃ¨me de logging existant** : Le package n'interfÃ¨re pas avec votre configuration de logging personnalisÃ©e
- **ContrÃ´le total** : Vous gardez le contrÃ´le complet sur la faÃ§on dont les logs sont formatÃ©s et oÃ¹ ils sont envoyÃ©s
- **IntÃ©gration facile** : Le package s'intÃ¨gre naturellement avec n'importe quel systÃ¨me de logging que vous utilisez

### IntÃ©gration avec un systÃ¨me de logging personnalisÃ©

Si vous utilisez un systÃ¨me de logging personnalisÃ© (par exemple une classe `Monitoring`), vous pouvez facilement intÃ©grer les logs de `dp-spark-utils` :

```python
import logging

# Exemple de classe Monitoring personnalisÃ©e (Ã  remplacer par votre propre implÃ©mentation)
class Monitoring:
    """Votre systÃ¨me de monitoring avec trame de logs spÃ©cifique."""

    def info(self, message):
        # Votre logique de logging info avec format personnalisÃ©
        pass

    def warning(self, message):
        # Votre logique de logging warning avec format personnalisÃ©
        pass

    def error(self, message):
        # Votre logique de logging error avec format personnalisÃ©
        pass


# Instancier votre systÃ¨me de logging personnalisÃ©
monitoring = Monitoring()


# CrÃ©er un handler personnalisÃ© pour rediriger vers votre systÃ¨me
class MonitoringHandler(logging.Handler):
    """
    Handler personnalisÃ© qui redirige vers votre systÃ¨me de monitoring.

    Args:
        monitoring_instance: Instance avec mÃ©thodes info(), warning(), error()
    """

    def __init__(self, monitoring_instance):
        super().__init__()
        self.monitoring = monitoring_instance

    def emit(self, record):
        try:
            log_message = self.format(record)
            if record.levelno >= logging.ERROR:
                self.monitoring.error(log_message)
            elif record.levelno >= logging.WARNING:
                self.monitoring.warning(log_message)
            else:
                self.monitoring.info(log_message)
        except Exception:
            self.handleError(record)


# Ajouter le handler aux loggers de dp-spark-utils
dp_logger = logging.getLogger('dp_spark_utils')
dp_logger.addHandler(MonitoringHandler(monitoring))
dp_logger.setLevel(logging.INFO)
```

### ContrÃ´ler le niveau de log

```python
import logging

# Activer les logs DEBUG pour tout le package
logging.getLogger('dp_spark_utils').setLevel(logging.DEBUG)

# Ou uniquement pour un module spÃ©cifique
logging.getLogger('dp_spark_utils.hdfs').setLevel(logging.DEBUG)

# DÃ©sactiver les logs du package
logging.getLogger('dp_spark_utils').setLevel(logging.CRITICAL)
```

### Exemple avec une configuration de logging standard

```python
import logging

# Configuration de base avec format personnalisÃ©
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Maintenant les logs de dp-spark-utils utiliseront cette configuration
from dp_spark_utils import check_file_exists
```

## ðŸ“„ Licence

MIT License

## ðŸ‘¥ Auteurs

Data Platform Team
