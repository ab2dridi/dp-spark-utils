===================\nUtilisation du package dp-spark-utils\n===================\n\nLe package **dp-spark-utils** a été conçu pour simplifier et automatiser les interactions avec Spark dans le cadre de l'analyse des données sur la plateforme de données.\n\n---\n\nInstallation\n-----------\n\nAssurez-vous que vous utilisez Python dans les versions 3.8, 3.9 ou 3.10, nécessaires pour la version supportée de PySpark.\nInstallez les dépendances avec la commande suivante :\n\n.. code-block:: bash\n\n   pip install -r requirements.txt\n\n---\n\nExemples d'utilisation\n----------------------\n\nVoici quelques exemples de fonctionnalités proposées par le package :\n\n1. Configuration automatique des `SparkSessions` :\n\n   .. code-block:: python\n\n      from dp_spark_utils import session_handler\n\n      spark = session_handler.get_spark_session(app_name="DemoApp")\n\n      # Votre analyse Spark commence ici\n      df = spark.read.csv("/chemin/vers/fichier.csv")\n      df.show()\n\n2. Gestion des exceptions Spark :\n\n   Une API pratique pour gérer proprement les erreurs communes dans Spark.\n\n3. Intégrations developpées pour améliorer la productivité\n\n   - Automatisation des écritures dans des tables Delta Lake\n   - Validation de données & nettoyage optimisé\n\n\n---\n\nContribution\n-----------\n\nNous accueillons toutes les contributions ! Assurez-vous d'exécuter les tests avant d'envoyer vos pull-requests.\n\n.. code-block:: bash\n\n   pytest\n
